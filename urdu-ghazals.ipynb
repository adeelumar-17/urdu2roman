{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13172583,"sourceType":"datasetVersion","datasetId":8347302}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: imports, seeds, paths, device\nimport os\nimport re\nimport time\nimport json\nimport math\nimport unicodedata\nimport random\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# Paths (adjust as needed)\nINPUT_DATA_ROOT = \"/kaggle/input/urdu2roman/dataset\"  # where your urdu/ and roman/ folders live\nWORKDIR = \"/kaggle/working\"\nDATA_DIR = os.path.join(WORKDIR, \"data\")\nMODELS_DIR = os.path.join(WORKDIR, \"models\")\nCHECKPOINT_DIR = os.path.join(MODELS_DIR, \"seq2seq_checkpoints\")\nos.makedirs(DATA_DIR, exist_ok=True)\nos.makedirs(MODELS_DIR, exist_ok=True)\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nprint(\"Working data dir:\", DATA_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T18:42:54.533087Z","iopub.execute_input":"2025-10-05T18:42:54.533763Z","iopub.status.idle":"2025-10-05T18:42:54.543676Z","shell.execute_reply.started":"2025-10-05T18:42:54.533718Z","shell.execute_reply":"2025-10-05T18:42:54.543141Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nWorking data dir: /kaggle/working/data\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 2: load Urdu/Roman files (line-wise) and build pairs.csv\nimport os\n\nURDU_DIR = os.path.join(INPUT_DATA_ROOT, \"urdu\")\nROMAN_DIR = os.path.join(INPUT_DATA_ROOT, \"roman\")\n\nif not (os.path.isdir(URDU_DIR) and os.path.isdir(ROMAN_DIR)):\n    raise FileNotFoundError(f\"Expected folders 'urdu' and 'roman' inside {INPUT_DATA_ROOT}. Found: {os.listdir(INPUT_DATA_ROOT)}\")\n\npairs = []\nfor fname in sorted(os.listdir(URDU_DIR)):\n    urdu_path = os.path.join(URDU_DIR, fname)\n    roman_path = os.path.join(ROMAN_DIR, fname)\n    if not os.path.isfile(roman_path):\n        print(f\"Skipping {fname}: missing roman file\")\n        continue\n\n    # Read both files; split into non-empty lines; pair by line index\n    with open(urdu_path, \"r\", encoding=\"utf-8\") as f1:\n        urdu_lines = [ln.strip() for ln in f1.read().splitlines() if ln.strip()]\n    with open(roman_path, \"r\", encoding=\"utf-8\") as f2:\n        roman_lines = [ln.strip() for ln in f2.read().splitlines() if ln.strip()]\n\n    # If lengths differ, pair up to min length. Usually poetry lines align.\n    n = min(len(urdu_lines), len(roman_lines))\n    if n == 0:\n        continue\n    for i in range(n):\n        pairs.append({\"urdu\": urdu_lines[i], \"roman\": roman_lines[i]})\n\ndf = pd.DataFrame(pairs)\npairs_csv = os.path.join(DATA_DIR, \"pairs.csv\")\ndf.to_csv(pairs_csv, index=False, encoding=\"utf-8-sig\")\nprint(f\"Built pairs.csv with {len(df)} pairs -> {pairs_csv}\")\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:43:08.647742Z","iopub.execute_input":"2025-10-05T06:43:08.648204Z","iopub.status.idle":"2025-10-05T06:43:25.749382Z","shell.execute_reply.started":"2025-10-05T06:43:08.648180Z","shell.execute_reply":"2025-10-05T06:43:25.748622Z"}},"outputs":[{"name":"stdout","text":"Built pairs.csv with 21003 pairs -> /kaggle/working/data/pairs.csv\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                          urdu                             roman\n0     Ø¢ Ú¯ÛŒØ§ Ù¾Ú¾Ø± Ø±Ù…Ø¶Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§     aa gayÄ phir ramazÄÃ± kyÄ hogÄ\n1    ÛØ§Ø¦Û’ Ø§Û’ Ù¾ÛŒØ± Ù…ØºØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§    haa.e ai pÄ«r-e-muÄ¡hÄÃ± kyÄ hogÄ\n2    Ø¨Ø§Øº Ø¬Ù†Øª Ù…ÛŒÚº Ø³Ù…Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§  bÄÄ¡h-e-jannat meÃ± samÄÃ± kyÄ hogÄ\n3  ØªÙˆ Ù†ÛÛŒÚº Ø¬Ø¨ ØªÙˆ ÙˆÛØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§    tÅ« nahÄ«Ã± jab to vahÄÃ± kyÄ hogÄ\n4  Ø®ÙˆØ´ ÙˆÛ ÛÙˆØªØ§ ÛÛ’ Ù…Ø±Û’ Ù†Ø§Ù„ÙˆÚº Ø³Û’   á¸³hush vo hotÄ hai mire nÄloÃ± se","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>urdu</th>\n      <th>roman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ø¢ Ú¯ÛŒØ§ Ù¾Ú¾Ø± Ø±Ù…Ø¶Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>aa gayÄ phir ramazÄÃ± kyÄ hogÄ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ÛØ§Ø¦Û’ Ø§Û’ Ù¾ÛŒØ± Ù…ØºØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>haa.e ai pÄ«r-e-muÄ¡hÄÃ± kyÄ hogÄ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ø¨Ø§Øº Ø¬Ù†Øª Ù…ÛŒÚº Ø³Ù…Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>bÄÄ¡h-e-jannat meÃ± samÄÃ± kyÄ hogÄ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ØªÙˆ Ù†ÛÛŒÚº Ø¬Ø¨ ØªÙˆ ÙˆÛØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>tÅ« nahÄ«Ã± jab to vahÄÃ± kyÄ hogÄ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ø®ÙˆØ´ ÙˆÛ ÛÙˆØªØ§ ÛÛ’ Ù…Ø±Û’ Ù†Ø§Ù„ÙˆÚº Ø³Û’</td>\n      <td>á¸³hush vo hotÄ hai mire nÄloÃ± se</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Cell 3: normalization / cleaning\nimport unicodedata, re\nfrom tqdm import tqdm\ntqdm.pandas()\n\ndef normalize_urdu(text):\n    text = \"\" if pd.isna(text) else str(text)\n    text = unicodedata.normalize(\"NFKC\", text)\n    text = text.replace(\"\\u0640\", \"\")  # tatweel\n    text = re.sub(r\"[\\u064B-\\u0652]\", \"\", text)  # tashkeel\n    text = re.sub(\"[\\u0622\\u0623\\u0625]\", \"\\u0627\", text)  # normalize alef\n    text = text.replace(\"ÙŠ\", \"ÛŒ\").replace(\"Ùƒ\", \"Ú©\")\n    text = re.sub(r\"[^\\S\\r\\n]+\", \" \", text).strip()\n    return text\n\ndef normalize_roman(text):\n    text = \"\" if pd.isna(text) else str(text)\n    text = unicodedata.normalize(\"NFKC\", text).lower()\n    text = text.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n    text = text.replace(\"â€™\", \"'\").replace(\"â€˜\", \"'\")\n    allow_punct = set(\" .,!?\\-\\'\\\":;()[]{}\")\n    out = []\n    for ch in text:\n        cat = unicodedata.category(ch)\n        if cat[0] in (\"L\", \"M\", \"N\") or ch in allow_punct:\n            out.append(ch)\n    text = \"\".join(out)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\npairs_csv = os.path.join(DATA_DIR, \"pairs.csv\")\nif not os.path.exists(pairs_csv):\n    raise FileNotFoundError(f\"Missing {pairs_csv}; run the previous cell to build it.\")\n\ndf = pd.read_csv(pairs_csv, encoding=\"utf-8\")\ndf[\"urdu_clean\"] = df[\"urdu\"].progress_apply(normalize_urdu)\ndf[\"roman_clean\"] = df[\"roman\"].progress_apply(normalize_roman)\n\nclean_csv = os.path.join(DATA_DIR, \"pairs_clean.csv\")\ndf.to_csv(clean_csv, index=False, encoding=\"utf-8-sig\")\nprint(\"Saved cleaned pairs to\", clean_csv)\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:43:36.135997Z","iopub.execute_input":"2025-10-05T06:43:36.136604Z","iopub.status.idle":"2025-10-05T06:43:36.773128Z","shell.execute_reply.started":"2025-10-05T06:43:36.136580Z","shell.execute_reply":"2025-10-05T06:43:36.772544Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21003/21003 [00:00<00:00, 130979.04it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21003/21003 [00:00<00:00, 75382.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved cleaned pairs to /kaggle/working/data/pairs_clean.csv\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                          urdu                             roman  \\\n0     Ø¢ Ú¯ÛŒØ§ Ù¾Ú¾Ø± Ø±Ù…Ø¶Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§     aa gayÄ phir ramazÄÃ± kyÄ hogÄ   \n1    ÛØ§Ø¦Û’ Ø§Û’ Ù¾ÛŒØ± Ù…ØºØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§    haa.e ai pÄ«r-e-muÄ¡hÄÃ± kyÄ hogÄ   \n2    Ø¨Ø§Øº Ø¬Ù†Øª Ù…ÛŒÚº Ø³Ù…Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§  bÄÄ¡h-e-jannat meÃ± samÄÃ± kyÄ hogÄ   \n3  ØªÙˆ Ù†ÛÛŒÚº Ø¬Ø¨ ØªÙˆ ÙˆÛØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§    tÅ« nahÄ«Ã± jab to vahÄÃ± kyÄ hogÄ   \n4  Ø®ÙˆØ´ ÙˆÛ ÛÙˆØªØ§ ÛÛ’ Ù…Ø±Û’ Ù†Ø§Ù„ÙˆÚº Ø³Û’   á¸³hush vo hotÄ hai mire nÄloÃ± se   \n\n                    urdu_clean                       roman_clean  \n0     Ø§ Ú¯ÛŒØ§ Ù¾Ú¾Ø± Ø±Ù…Ø¶Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§     aa gayÄ phir ramazÄÃ± kyÄ hogÄ  \n1    ÛØ§Ø¦Û’ Ø§Û’ Ù¾ÛŒØ± Ù…ØºØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§    haa.e ai pÄ«r-e-muÄ¡hÄÃ± kyÄ hogÄ  \n2    Ø¨Ø§Øº Ø¬Ù†Øª Ù…ÛŒÚº Ø³Ù…Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§  bÄÄ¡h-e-jannat meÃ± samÄÃ± kyÄ hogÄ  \n3  ØªÙˆ Ù†ÛÛŒÚº Ø¬Ø¨ ØªÙˆ ÙˆÛØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§    tÅ« nahÄ«Ã± jab to vahÄÃ± kyÄ hogÄ  \n4  Ø®ÙˆØ´ ÙˆÛ ÛÙˆØªØ§ ÛÛ’ Ù…Ø±Û’ Ù†Ø§Ù„ÙˆÚº Ø³Û’   á¸³hush vo hotÄ hai mire nÄloÃ± se  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>urdu</th>\n      <th>roman</th>\n      <th>urdu_clean</th>\n      <th>roman_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ø¢ Ú¯ÛŒØ§ Ù¾Ú¾Ø± Ø±Ù…Ø¶Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>aa gayÄ phir ramazÄÃ± kyÄ hogÄ</td>\n      <td>Ø§ Ú¯ÛŒØ§ Ù¾Ú¾Ø± Ø±Ù…Ø¶Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>aa gayÄ phir ramazÄÃ± kyÄ hogÄ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ÛØ§Ø¦Û’ Ø§Û’ Ù¾ÛŒØ± Ù…ØºØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>haa.e ai pÄ«r-e-muÄ¡hÄÃ± kyÄ hogÄ</td>\n      <td>ÛØ§Ø¦Û’ Ø§Û’ Ù¾ÛŒØ± Ù…ØºØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>haa.e ai pÄ«r-e-muÄ¡hÄÃ± kyÄ hogÄ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ø¨Ø§Øº Ø¬Ù†Øª Ù…ÛŒÚº Ø³Ù…Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>bÄÄ¡h-e-jannat meÃ± samÄÃ± kyÄ hogÄ</td>\n      <td>Ø¨Ø§Øº Ø¬Ù†Øª Ù…ÛŒÚº Ø³Ù…Ø§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>bÄÄ¡h-e-jannat meÃ± samÄÃ± kyÄ hogÄ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ØªÙˆ Ù†ÛÛŒÚº Ø¬Ø¨ ØªÙˆ ÙˆÛØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>tÅ« nahÄ«Ã± jab to vahÄÃ± kyÄ hogÄ</td>\n      <td>ØªÙˆ Ù†ÛÛŒÚº Ø¬Ø¨ ØªÙˆ ÙˆÛØ§Úº Ú©ÛŒØ§ ÛÙˆÚ¯Ø§</td>\n      <td>tÅ« nahÄ«Ã± jab to vahÄÃ± kyÄ hogÄ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ø®ÙˆØ´ ÙˆÛ ÛÙˆØªØ§ ÛÛ’ Ù…Ø±Û’ Ù†Ø§Ù„ÙˆÚº Ø³Û’</td>\n      <td>á¸³hush vo hotÄ hai mire nÄloÃ± se</td>\n      <td>Ø®ÙˆØ´ ÙˆÛ ÛÙˆØªØ§ ÛÛ’ Ù…Ø±Û’ Ù†Ø§Ù„ÙˆÚº Ø³Û’</td>\n      <td>á¸³hush vo hotÄ hai mire nÄloÃ± se</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Cell 4: split dataset into train/val/test (50/25/25)\nfrom sklearn.model_selection import train_test_split\nimport numpy as np, os\n\nclean_csv = os.path.join(DATA_DIR, \"pairs_clean.csv\")\ndf = pd.read_csv(clean_csv, encoding=\"utf-8\")\n\n# Optional: stratify by length buckets to ensure splits have similar length distribution\nlengths = df[\"urdu_clean\"].astype(str).apply(len)\nbins = np.minimum((lengths // 20).astype(int), 9)  # coarse buckets\ndf[\"len_bucket\"] = bins\n\ntrain_df, rest = train_test_split(df, test_size=0.5, random_state=SEED, stratify=df[\"len_bucket\"])\nval_df, test_df = train_test_split(rest, test_size=0.5, random_state=SEED, stratify=rest[\"len_bucket\"])\n\nfor name, d in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n    path = os.path.join(DATA_DIR, f\"{name}.csv\")\n    d.to_csv(path, index=False, encoding=\"utf-8-sig\")\n    print(f\"Saved {name}.csv -> {path} ({len(d)} rows)\")\n\n# clean up temporary column\nfor d in (train_df, val_df, test_df):\n    if \"len_bucket\" in d.columns:\n        d.drop(columns=[\"len_bucket\"], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:45:24.844202Z","iopub.execute_input":"2025-10-05T06:45:24.844814Z","iopub.status.idle":"2025-10-05T06:45:25.116471Z","shell.execute_reply.started":"2025-10-05T06:45:24.844790Z","shell.execute_reply":"2025-10-05T06:45:25.115725Z"}},"outputs":[{"name":"stdout","text":"Saved train.csv -> /kaggle/working/data/train.csv (10501 rows)\nSaved val.csv -> /kaggle/working/data/val.csv (5251 rows)\nSaved test.csv -> /kaggle/working/data/test.csv (5251 rows)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Cell 5: character-level tokenizer (recommended for transliteration)\nimport json, os\n\nclass CharTokenizer:\n    def __init__(self, specials=[\"<pad>\",\"<sos>\",\"<eos>\",\"<unk>\"]):\n        self.specials = specials\n        self.vocab = None\n        self.inv_vocab = None\n\n    def build(self, texts):\n        chars = set()\n        for t in texts:\n            for ch in str(t):\n                chars.add(ch)\n        chars = sorted(chars)\n        tokens = list(self.specials) + chars\n        self.vocab = {tok:i for i,tok in enumerate(tokens)}\n        self.inv_vocab = {i:tok for tok,i in self.vocab.items()}\n        return self.vocab\n\n    def save(self, prefix):\n        with open(prefix + \"_vocab.json\",\"w\",encoding=\"utf-8\") as f:\n            json.dump(self.vocab, f, ensure_ascii=False, indent=2)\n\n    def load(self, prefix):\n        with open(prefix + \"_vocab.json\",\"r\",encoding=\"utf-8\") as f:\n            self.vocab = json.load(f)\n            self.inv_vocab = {int(i):tok for tok,i in enumerate(self.vocab)} if False else {v:int(k) for k,v in self.vocab.items()} \n            # fix: build inv_vocab properly\n            self.inv_vocab = {int(v):k for k,v in self.vocab.items()}\n\n    def encode(self, text):\n        return [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in str(text)]\n\n    def decode(self, ids, stop_at_eos=True):\n        out = []\n        for i in ids:\n            tok = self.inv_vocab.get(int(i), \"<unk>\")\n            if tok in (\"<pad>\",\"<sos>\"):\n                continue\n            if tok == \"<eos>\" and stop_at_eos:\n                break\n            out.append(tok)\n        return \"\".join(out)\n\n# Build tokenizers from training data\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"), encoding=\"utf-8\")\nchar_tok_src = CharTokenizer()  # Urdu\nchar_tok_tgt = CharTokenizer()  # Roman\n\nprint(\"Building src (Urdu) tokenizer...\")\nchar_tok_src.build(train_df[\"urdu_clean\"].astype(str).tolist())\nprint(\"Building tgt (Roman) tokenizer...\")\nchar_tok_tgt.build(train_df[\"roman_clean\"].astype(str).tolist())\n\nchar_tok_src.save(os.path.join(MODELS_DIR, \"char_urdu\"))\nchar_tok_tgt.save(os.path.join(MODELS_DIR, \"char_roman\"))\nprint(\"Saved char tokenizers to\", MODELS_DIR)\n\n# Expose vocab sizes & pad/sos/eos ids\nSRC_VOCAB_SIZE = len(char_tok_src.vocab)\nTGT_VOCAB_SIZE = len(char_tok_tgt.vocab)\nSRC_PAD = int(char_tok_src.vocab[\"<pad>\"])\nTGT_PAD = int(char_tok_tgt.vocab[\"<pad>\"])\nTGT_SOS = int(char_tok_tgt.vocab[\"<sos>\"])\nTGT_EOS = int(char_tok_tgt.vocab[\"<eos>\"])\n\nprint(\"SRC_VOCAB_SIZE\", SRC_VOCAB_SIZE, \"TGT_VOCAB_SIZE\", TGT_VOCAB_SIZE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T18:45:17.515641Z","iopub.execute_input":"2025-10-05T18:45:17.516427Z","iopub.status.idle":"2025-10-05T18:45:17.620043Z","shell.execute_reply.started":"2025-10-05T18:45:17.516401Z","shell.execute_reply":"2025-10-05T18:45:17.619326Z"}},"outputs":[{"name":"stdout","text":"Building src (Urdu) tokenizer...\nBuilding tgt (Roman) tokenizer...\nSaved char tokenizers to /kaggle/working/models\nSRC_VOCAB_SIZE 56 TGT_VOCAB_SIZE 44\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Cell 6: Dataset and DataLoader (char tokens)\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass UrduRomanCharDataset(Dataset):\n    def __init__(self, df, src_tok, tgt_tok, max_src_len=200, max_tgt_len=250):\n        self.df = df.reset_index(drop=True)\n        self.src_tok = src_tok\n        self.tgt_tok = tgt_tok\n        self.max_src_len = max_src_len\n        self.max_tgt_len = max_tgt_len\n        self.src_pad = int(src_tok.vocab[\"<pad>\"])\n        self.tgt_pad = int(tgt_tok.vocab[\"<pad>\"])\n        self.tgt_sos = int(tgt_tok.vocab[\"<sos>\"])\n        self.tgt_eos = int(tgt_tok.vocab[\"<eos>\"])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        src = str(row[\"urdu_clean\"])\n        tgt = str(row[\"roman_clean\"])\n        src_ids = self.src_tok.encode(src)[:self.max_src_len]\n        tgt_ids = self.tgt_tok.encode(tgt)[:(self.max_tgt_len-1)]  # leave space for eos\n        tgt_in = [self.tgt_sos] + tgt_ids\n        tgt_out = tgt_ids + [self.tgt_eos]\n        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_in, dtype=torch.long), torch.tensor(tgt_out, dtype=torch.long)\n\ndef collate_fn(batch):\n    srcs, tgts_in, tgts_out = zip(*batch)\n    src_pad = SRC_PAD\n    tgt_pad = TGT_PAD\n    src_lens = torch.tensor([s.size(0) for s in srcs], dtype=torch.long)\n    tgt_lens = torch.tensor([t.size(0) for t in tgts_in], dtype=torch.long)\n    src_padded = pad_sequence(srcs, batch_first=True, padding_value=src_pad)\n    tgt_in_padded = pad_sequence(tgts_in, batch_first=True, padding_value=tgt_pad)\n    tgt_out_padded = pad_sequence(tgts_out, batch_first=True, padding_value=tgt_pad)\n    src_mask = (src_padded != src_pad)\n    tgt_mask = (tgt_in_padded != tgt_pad)\n    return src_padded, src_lens, src_mask, tgt_in_padded, tgt_out_padded, tgt_lens\n\n# build datasets & loaders\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"), encoding=\"utf-8\")\nval_df   = pd.read_csv(os.path.join(DATA_DIR, \"val.csv\"), encoding=\"utf-8\")\ntest_df  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"), encoding=\"utf-8\")\n\n# compute reasonable max lens (optional)\ndef compute_stats(tok, texts, n_samples=2000):\n    lens = [len(tok.encode(str(t))) for t in texts[:n_samples]]\n    arr = np.array(lens)\n    return int(np.percentile(arr, 99))\n\nMAX_SRC_LEN = min(200, compute_stats(char_tok_src, train_df[\"urdu_clean\"].astype(str).tolist(), n_samples=2000) + 5)\nMAX_TGT_LEN = min(250, compute_stats(char_tok_tgt, train_df[\"roman_clean\"].astype(str).tolist(), n_samples=2000) + 5)\n\ntrain_dataset = UrduRomanCharDataset(train_df, char_tok_src, char_tok_tgt, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\nval_dataset   = UrduRomanCharDataset(val_df,   char_tok_src, char_tok_tgt, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\ntest_dataset  = UrduRomanCharDataset(test_df,  char_tok_src, char_tok_tgt, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2)\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2)\n\nprint(\"DataLoaders ready. Example batch shapes:\")\nb = next(iter(train_loader))\nprint(\"src:\", b[0].shape, \"tgt_in:\", b[3].shape, \"tgt_out:\", b[4].shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:45:36.565287Z","iopub.execute_input":"2025-10-05T06:45:36.565856Z","iopub.status.idle":"2025-10-05T06:45:36.778605Z","shell.execute_reply.started":"2025-10-05T06:45:36.565831Z","shell.execute_reply":"2025-10-05T06:45:36.777926Z"}},"outputs":[{"name":"stdout","text":"DataLoaders ready. Example batch shapes:\nsrc: torch.Size([32, 53]) tgt_in: torch.Size([32, 64]) tgt_out: torch.Size([32, 64])\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Cell 7: model definitions\nEMB_DIM = 128\nHID_DIM = 256\nENC_LAYERS = 2\nDEC_LAYERS = 2\nDROPOUT = 0.2\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.2, pad_idx=0):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n        self.rnn = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, num_layers=n_layers,\n                           dropout=dropout if n_layers>1 else 0.0, bidirectional=True, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.n_layers = n_layers\n        self.hid_dim = hid_dim\n\n    def forward(self, src, src_lengths):\n        embedded = self.embedding(src)\n        embedded = self.dropout(embedded)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n        packed_outputs, (h_n, c_n) = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n        return outputs, h_n, c_n\n\nclass LuongAttention(nn.Module):\n    def __init__(self, enc_dim, dec_dim):\n        super().__init__()\n        self.W = nn.Linear(dec_dim, enc_dim, bias=False)\n\n    def forward(self, dec_hidden, enc_outputs, mask=None):\n        proj = self.W(dec_hidden).unsqueeze(2)\n        scores = torch.bmm(enc_outputs, proj).squeeze(2)\n        if mask is not None:\n            scores = scores.masked_fill(~mask, -1e9)\n        attn_weights = F.softmax(scores, dim=1)\n        context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs).squeeze(1)\n        return attn_weights, context\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers=2, dropout=0.2, pad_idx=0):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n        self.attn = LuongAttention(enc_dim=enc_hid_dim*2, dec_dim=dec_hid_dim)\n        self.rnn = nn.LSTM(input_size=emb_dim + enc_hid_dim*2, hidden_size=dec_hid_dim,\n                           num_layers=n_layers, batch_first=True, dropout=dropout if n_layers>1 else 0.0)\n        self.fc_out = nn.Linear(dec_hid_dim + enc_hid_dim*2 + emb_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.n_layers = n_layers\n        self.dec_hid_dim = dec_hid_dim\n\n    def forward_step(self, input_tok, last_hidden, last_cell, enc_outputs, enc_mask):\n        emb = self.embedding(input_tok).unsqueeze(1)\n        emb = self.dropout(emb)\n        dec_top_hidden = last_hidden[-1]\n        attn_weights, context = self.attn(dec_top_hidden, enc_outputs, enc_mask)\n        rnn_input = torch.cat([emb, context.unsqueeze(1)], dim=2)\n        output, (hidden, cell) = self.rnn(rnn_input, (last_hidden, last_cell))\n        output = output.squeeze(1)\n        emb_s = emb.squeeze(1)\n        concat = torch.cat([output, context, emb_s], dim=1)\n        logits = self.fc_out(concat)\n        return logits, hidden, cell, attn_weights\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, enc_hid_dim, dec_hid_dim, dec_n_layers):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.dec_n_layers = dec_n_layers\n        enc_total = encoder.n_layers * 2 * enc_hid_dim\n        self.enc2dec_h = nn.Linear(enc_total, dec_n_layers * dec_hid_dim)\n        self.enc2dec_c = nn.Linear(enc_total, dec_n_layers * dec_hid_dim)\n\n    def forward(self, src, src_lens, tgt_in, teacher_forcing_ratio=0.5):\n        B = src.size(0)\n        max_tgt_len = tgt_in.size(1)\n        enc_outputs, enc_h, enc_c = self.encoder(src, src_lens)\n        enc_mask = (src != SRC_PAD)\n        enc_h_flat = enc_h.permute(1,0,2).contiguous().view(B, -1)\n        enc_c_flat = enc_c.permute(1,0,2).contiguous().view(B, -1)\n        dec_h_flat = self.enc2dec_h(enc_h_flat)\n        dec_c_flat = self.enc2dec_c(enc_c_flat)\n        dec_h = dec_h_flat.view(self.dec_n_layers, B, self.dec_hid_dim).contiguous()\n        dec_c = dec_c_flat.view(self.dec_n_layers, B, self.dec_hid_dim).contiguous()\n        outputs = torch.zeros(B, max_tgt_len, TGT_VOCAB_SIZE, device=src.device)\n        input_tok = tgt_in[:, 0]\n        for t in range(1, max_tgt_len):\n            logits, dec_h, dec_c, attn = self.decoder.forward_step(input_tok, dec_h, dec_c, enc_outputs, enc_mask)\n            outputs[:, t, :] = logits\n            teacher_force = (random.random() < teacher_forcing_ratio)\n            if teacher_force:\n                input_tok = tgt_in[:, t]\n            else:\n                input_tok = logits.argmax(dim=1)\n        return outputs\n\n    def greedy_decode(self, src, src_lens, max_tgt_len=120):\n        B = src.size(0)\n        enc_outputs, enc_h, enc_c = self.encoder(src, src_lens)\n        enc_mask = (src != SRC_PAD)\n        enc_h_flat = enc_h.permute(1,0,2).contiguous().view(B, -1)\n        enc_c_flat = enc_c.permute(1,0,2).contiguous().view(B, -1)\n        dec_h_flat = self.enc2dec_h(enc_h_flat)\n        dec_c_flat = self.enc2dec_c(enc_c_flat)\n        dec_h = dec_h_flat.view(self.dec_n_layers, B, self.dec_hid_dim).contiguous()\n        dec_c = dec_c_flat.view(self.dec_n_layers, B, self.dec_hid_dim).contiguous()\n        preds = torch.full((B, max_tgt_len), TGT_PAD, dtype=torch.long, device=src.device)\n        input_tok = torch.full((B,), TGT_SOS, dtype=torch.long, device=src.device)\n        finished = torch.zeros(B, dtype=torch.bool, device=src.device)\n        for t in range(max_tgt_len):\n            logits, dec_h, dec_c, attn = self.decoder.forward_step(input_tok, dec_h, dec_c, enc_outputs, enc_mask)\n            next_tok = logits.argmax(dim=1)\n            preds[:, t] = next_tok\n            finished = finished | (next_tok == TGT_EOS)\n            if finished.all():\n                break\n            input_tok = next_tok\n        return preds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T18:44:28.227557Z","iopub.execute_input":"2025-10-05T18:44:28.228188Z","iopub.status.idle":"2025-10-05T18:44:28.245899Z","shell.execute_reply.started":"2025-10-05T18:44:28.228162Z","shell.execute_reply":"2025-10-05T18:44:28.245188Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Cell 8: construct model, optimizer, criterion\nenc = Encoder(input_dim=SRC_VOCAB_SIZE, emb_dim=EMB_DIM, hid_dim=HID_DIM, n_layers=ENC_LAYERS, dropout=DROPOUT, pad_idx=SRC_PAD)\ndec = Decoder(output_dim=TGT_VOCAB_SIZE, emb_dim=EMB_DIM, enc_hid_dim=HID_DIM, dec_hid_dim=HID_DIM, n_layers=DEC_LAYERS, dropout=DROPOUT, pad_idx=TGT_PAD)\nmodel = Seq2Seq(enc, dec, enc_hid_dim=HID_DIM, dec_hid_dim=HID_DIM, dec_n_layers=DEC_LAYERS).to(device)\nprint(\"Model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n\ncriterion = nn.CrossEntropyLoss(ignore_index=TGT_PAD, reduction=\"sum\")\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\nprint(\"Current LR:\", scheduler.get_last_lr())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T18:45:35.616028Z","iopub.execute_input":"2025-10-05T18:45:35.616696Z","iopub.status.idle":"2025-10-05T18:45:38.351521Z","shell.execute_reply.started":"2025-10-05T18:45:35.616668Z","shell.execute_reply":"2025-10-05T18:45:38.350939Z"}},"outputs":[{"name":"stdout","text":"Model parameters: 5046316\nCurrent LR: [0.0003]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 9: helpers for training & evaluation\nimport math\n\ndef compute_loss_and_tokens(preds, tgt_out, pad_idx=TGT_PAD):\n    B, T, V = preds.size()\n    preds_flat = preds.reshape(-1, V)\n    tgt_flat = tgt_out.reshape(-1)\n    loss = criterion(preds_flat, tgt_flat)\n    n_tokens = (tgt_out != pad_idx).sum().item()\n    return loss, n_tokens\n\ndef _ngrams(seq, n):\n    return [tuple(seq[i:i+n]) for i in range(len(seq)-n+1)] if len(seq) >= n else []\n\ndef corpus_bleu(references, hypotheses, max_n=4):\n    matches_by_order = [0]*max_n\n    possible_by_order = [0]*max_n\n    ref_len = 0\n    hyp_len = 0\n    for ref, hyp in zip(references, hypotheses):\n        ref_len += len(ref)\n        hyp_len += len(hyp)\n        for n in range(1, max_n+1):\n            ref_ngrams = Counter(_ngrams(ref, n))\n            hyp_ngrams = Counter(_ngrams(hyp, n))\n            overlap = sum((hyp_ngrams & ref_ngrams).values())\n            matches_by_order[n-1] += overlap\n            possible_by_order[n-1] += max(0, len(hyp)-n+1)\n    precisions = []\n    for i in range(max_n):\n        if possible_by_order[i] == 0:\n            precisions.append(0.0)\n        else:\n            precisions.append(matches_by_order[i] / possible_by_order[i])\n    smooth = 1e-9\n    log_prec = sum((1.0/max_n) * math.log(max(p, smooth)) for p in precisions)\n    bp = math.exp(1 - ref_len/hyp_len) if hyp_len < ref_len and hyp_len>0 else 1.0\n    bleu = bp * math.exp(log_prec)\n    return bleu * 100\n\ndef cer(refs, hyps):\n    total = 0\n    edits = 0\n    for r, h in zip(refs, hyps):\n        a = list(r)\n        b = list(h)\n        n, m = len(a), len(b)\n        dp = np.zeros((n+1, m+1), dtype=int)\n        for i in range(n+1):\n            dp[i,0] = i\n        for j in range(m+1):\n            dp[0,j] = j\n        for i in range(1,n+1):\n            for j in range(1,m+1):\n                if a[i-1] == b[j-1]:\n                    dp[i,j] = dp[i-1,j-1]\n                else:\n                    dp[i,j] = 1 + min(dp[i-1,j], dp[i,j-1], dp[i-1,j-1])\n        edits += dp[n,m]\n        total += n\n    return edits / max(1, total)\n\n# decode helpers using char_tok_tgt.inv_vocab\ndef ids_to_string_tokenwise(id_seq):\n    return char_tok_tgt.decode(id_seq, stop_at_eos=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:45:59.714016Z","iopub.execute_input":"2025-10-05T06:45:59.714292Z","iopub.status.idle":"2025-10-05T06:45:59.725838Z","shell.execute_reply.started":"2025-10-05T06:45:59.714272Z","shell.execute_reply":"2025-10-05T06:45:59.724949Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Cell 10: training loop\nNUM_EPOCHS = 30\nbest_val_bleu = -1.0\npatience = 5\nno_improve = 0\n\ndef train_epoch(model, loader, optimizer, epoch, teacher_forcing_ratio=0.5, clip=1.0):\n    model.train()\n    total_loss = 0.0\n    total_tokens = 0\n    t0 = time.time()\n    for i, batch in enumerate(loader):\n        src_padded, src_lens, src_mask, tgt_in_padded, tgt_out_padded, tgt_lens = batch\n        src_padded = src_padded.to(device)\n        src_lens = src_lens.to(device)\n        tgt_in_padded = tgt_in_padded.to(device)\n        tgt_out_padded = tgt_out_padded.to(device)\n        optimizer.zero_grad()\n        outputs = model(src_padded, src_lens, tgt_in_padded, teacher_forcing_ratio=teacher_forcing_ratio)\n        preds = outputs[:,1:,:]\n        targets = tgt_out_padded[:, :-1]\n        loss, n_tokens = compute_loss_and_tokens(preds, targets, pad_idx=TGT_PAD)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        total_loss += loss.item()\n        total_tokens += n_tokens\n    avg_loss = total_loss / max(1, total_tokens)\n    return avg_loss\n\ndef evaluate(model, loader, n_samples=200, max_gen_len=MAX_TGT_LEN):\n    model.eval()\n    references = []\n    hypotheses = []\n    total_loss = 0.0\n    total_tokens = 0\n    with torch.no_grad():\n        for i, batch in enumerate(loader):\n            src_padded, src_lens, src_mask, tgt_in_padded, tgt_out_padded, tgt_lens = batch\n            src_padded = src_padded.to(device)\n            src_lens = src_lens.to(device)\n            tgt_in_padded = tgt_in_padded.to(device)\n            tgt_out_padded = tgt_out_padded.to(device)\n            outputs = model(src_padded, src_lens, tgt_in_padded, teacher_forcing_ratio=0.0)\n            preds = outputs[:,1:,:]\n            targets = tgt_out_padded[:, :-1]\n            loss, n_tokens = compute_loss_and_tokens(preds, targets, pad_idx=TGT_PAD)\n            total_loss += loss.item()\n            total_tokens += n_tokens\n            # sample generated outputs\n            if len(hypotheses) < n_samples:\n                gen = model.greedy_decode(src_padded, src_lens, max_tgt_len=max_gen_len)\n                B = gen.size(0)\n                for b in range(B):\n                    ref_ids = tgt_out_padded[b,:tgt_lens[b]].tolist()\n                    hyp_ids = gen[b].tolist()\n                    ref_str = char_tok_tgt.decode(ref_ids)\n                    hyp_str = char_tok_tgt.decode(hyp_ids)\n                    references.append(list(ref_str))\n                    hypotheses.append(list(hyp_str))\n    avg_loss = total_loss / max(1, total_tokens)\n    bleu_score = corpus_bleu(references, hypotheses) if len(hypotheses)>0 else 0.0\n    cer_score = cer([\"\".join(r) for r in references], [\"\".join(h) for h in hypotheses]) if len(hypotheses)>0 else 1.0\n    return avg_loss, bleu_score, cer_score\n\n# teacher forcing schedule (linear decay)\ntf_start = 1.0\ntf_end = 0.5\n\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nfor epoch in range(1, NUM_EPOCHS+1):\n    tf_ratio = tf_start + (tf_end - tf_start) * (epoch-1)/(NUM_EPOCHS-1)\n    t0 = time.time()\n    train_loss = train_epoch(model, train_loader, optimizer, epoch, teacher_forcing_ratio=tf_ratio, clip=1.0)\n    val_loss, val_bleu, val_cer = evaluate(model, val_loader, n_samples=200, max_gen_len=MAX_TGT_LEN)\n    scheduler.step(val_loss)\n    print(f\"Epoch {epoch} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | val_bleu {val_bleu:.2f} | val_cer {val_cer:.4f} | tf {tf_ratio:.3f} | time {time.time()-t0:.1f}s\")\n\n    # save checkpoint if improved\n    if val_bleu > best_val_bleu:\n        best_val_bleu = val_bleu\n        no_improve = 0\n        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"best_model_epoch{epoch}_bleu{val_bleu:.2f}.pt\")\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"val_bleu\": val_bleu,\n            \"val_cer\": val_cer,\n            \"char_src_vocab\": char_tok_src.vocab,\n            \"char_tgt_vocab\": char_tok_tgt.vocab\n        }, ckpt_path)\n        print(\"Saved checkpoint:\", ckpt_path)\n    else:\n        no_improve += 1\n    if no_improve >= patience:\n        print(f\"No improvement for {patience} epochs. Early stopping.\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:46:08.377611Z","iopub.execute_input":"2025-10-05T06:46:08.378409Z","iopub.status.idle":"2025-10-05T07:11:22.974313Z","shell.execute_reply.started":"2025-10-05T06:46:08.378382Z","shell.execute_reply":"2025-10-05T07:11:22.973276Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | train_loss 1.9648 | val_loss 3.3297 | val_bleu 53.75 | val_cer 0.3340 | tf 1.000 | time 60.9s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch1_bleu53.75.pt\nEpoch 2 | train_loss 0.6860 | val_loss 3.4103 | val_bleu 69.85 | val_cer 0.1814 | tf 0.983 | time 60.5s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch2_bleu69.85.pt\nEpoch 3 | train_loss 0.4288 | val_loss 3.2232 | val_bleu 79.76 | val_cer 0.1206 | tf 0.966 | time 60.6s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch3_bleu79.76.pt\nEpoch 4 | train_loss 0.3231 | val_loss 3.0013 | val_bleu 85.13 | val_cer 0.0918 | tf 0.948 | time 59.7s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch4_bleu85.13.pt\nEpoch 5 | train_loss 0.2637 | val_loss 2.8016 | val_bleu 87.12 | val_cer 0.0804 | tf 0.931 | time 60.0s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch5_bleu87.12.pt\nEpoch 6 | train_loss 0.2224 | val_loss 2.7152 | val_bleu 88.07 | val_cer 0.0729 | tf 0.914 | time 59.7s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch6_bleu88.07.pt\nEpoch 7 | train_loss 0.2003 | val_loss 2.4460 | val_bleu 90.19 | val_cer 0.0609 | tf 0.897 | time 60.7s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch7_bleu90.19.pt\nEpoch 8 | train_loss 0.1795 | val_loss 2.4220 | val_bleu 91.02 | val_cer 0.0558 | tf 0.879 | time 60.3s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch8_bleu91.02.pt\nEpoch 9 | train_loss 0.1655 | val_loss 2.2648 | val_bleu 91.57 | val_cer 0.0519 | tf 0.862 | time 61.4s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch9_bleu91.57.pt\nEpoch 10 | train_loss 0.1527 | val_loss 2.1584 | val_bleu 91.50 | val_cer 0.0511 | tf 0.845 | time 60.7s\nEpoch 11 | train_loss 0.1392 | val_loss 2.2667 | val_bleu 91.69 | val_cer 0.0524 | tf 0.828 | time 60.6s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch11_bleu91.69.pt\nEpoch 12 | train_loss 0.1346 | val_loss 1.9849 | val_bleu 92.50 | val_cer 0.0477 | tf 0.810 | time 61.1s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch12_bleu92.50.pt\nEpoch 13 | train_loss 0.1233 | val_loss 1.9314 | val_bleu 93.49 | val_cer 0.0388 | tf 0.793 | time 59.7s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch13_bleu93.49.pt\nEpoch 14 | train_loss 0.1193 | val_loss 1.8248 | val_bleu 93.43 | val_cer 0.0406 | tf 0.776 | time 60.5s\nEpoch 15 | train_loss 0.1151 | val_loss 1.8270 | val_bleu 93.79 | val_cer 0.0386 | tf 0.759 | time 60.1s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch15_bleu93.79.pt\nEpoch 16 | train_loss 0.1100 | val_loss 1.8224 | val_bleu 94.28 | val_cer 0.0373 | tf 0.741 | time 60.6s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch16_bleu94.28.pt\nEpoch 17 | train_loss 0.1087 | val_loss 1.7019 | val_bleu 94.50 | val_cer 0.0344 | tf 0.724 | time 61.5s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch17_bleu94.50.pt\nEpoch 18 | train_loss 0.1004 | val_loss 1.7240 | val_bleu 94.43 | val_cer 0.0376 | tf 0.707 | time 60.6s\nEpoch 19 | train_loss 0.1032 | val_loss 1.6951 | val_bleu 94.48 | val_cer 0.0345 | tf 0.690 | time 61.2s\nEpoch 20 | train_loss 0.0963 | val_loss 1.6880 | val_bleu 95.24 | val_cer 0.0307 | tf 0.672 | time 60.8s\nSaved checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch20_bleu95.24.pt\nEpoch 21 | train_loss 0.0994 | val_loss 1.7203 | val_bleu 94.54 | val_cer 0.0351 | tf 0.655 | time 60.7s\nEpoch 22 | train_loss 0.0952 | val_loss 1.6280 | val_bleu 94.78 | val_cer 0.0329 | tf 0.638 | time 60.6s\nEpoch 23 | train_loss 0.0938 | val_loss 1.5547 | val_bleu 94.98 | val_cer 0.0326 | tf 0.621 | time 60.3s\nEpoch 24 | train_loss 0.0882 | val_loss 1.5752 | val_bleu 95.21 | val_cer 0.0331 | tf 0.603 | time 60.5s\nEpoch 25 | train_loss 0.0875 | val_loss 1.5280 | val_bleu 95.13 | val_cer 0.0315 | tf 0.586 | time 59.8s\nNo improvement for 5 epochs. Early stopping.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# === Rebuild model and load best checkpoint ===\nimport torch\nimport os\nimport glob\nimport torch.serialization\nfrom numpy.core.multiarray import scalar\n\n# Allow safe numpy unpickling (needed for old checkpoints)\ntorch.serialization.add_safe_globals([scalar])\n\n# Correct paths based on your actual structure\nWORKDIR = \"/kaggle/working\"\nMODELS_DIR = os.path.join(WORKDIR, \"models\")\nCHECKPOINT_DIR = os.path.join(MODELS_DIR, \"seq2seq_checkpoints\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Find best checkpoint\nckpts = sorted(glob.glob(os.path.join(CHECKPOINT_DIR, \"best_model_epoch*_bleu*.pt\")))\nif not ckpts:\n    raise FileNotFoundError(f\"No checkpoints found in {CHECKPOINT_DIR}\")\nbest_ckpt = ckpts[-1]\nprint(\"Loading checkpoint:\", best_ckpt)\n\n# Load safely (PyTorch â‰¥ 2.6 fix)\ncheckpoint = torch.load(best_ckpt, map_location=device, weights_only=False)\n\n# --- Rebuild model (must match training settings) ---\nenc = Encoder(\n    input_dim=SRC_VOCAB_SIZE, emb_dim=EMB_DIM, hid_dim=HID_DIM,\n    n_layers=ENC_LAYERS, dropout=DROPOUT, pad_idx=SRC_PAD\n)\ndec = Decoder(\n    output_dim=TGT_VOCAB_SIZE, emb_dim=EMB_DIM, enc_hid_dim=HID_DIM,\n    dec_hid_dim=HID_DIM, n_layers=DEC_LAYERS, dropout=DROPOUT, pad_idx=TGT_PAD\n)\nmodel = Seq2Seq(enc, dec, enc_hid_dim=HID_DIM, dec_hid_dim=HID_DIM,\n                dec_n_layers=DEC_LAYERS).to(device)\n\nmodel.load_state_dict(checkpoint[\"model_state\"])\nmodel.eval()\nprint(f\"âœ… Model loaded successfully from {best_ckpt}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T18:45:44.411685Z","iopub.execute_input":"2025-10-05T18:45:44.412564Z","iopub.status.idle":"2025-10-05T18:45:44.510066Z","shell.execute_reply.started":"2025-10-05T18:45:44.412538Z","shell.execute_reply":"2025-10-05T18:45:44.509362Z"}},"outputs":[{"name":"stdout","text":"Loading checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch9_bleu91.57.pt\nâœ… Model loaded successfully from /kaggle/working/models/seq2seq_checkpoints/best_model_epoch9_bleu91.57.pt\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ============================\n# âœ… Cell 11 â€” Final Evaluation & Inference\n# ============================\n\nimport os\nimport torch\nimport pandas as pd\n\n# Define data + model directories\nBASE_DIR = \"/kaggle/working\"  # adjust if different\nDATA_DIR = os.path.join(BASE_DIR, \"dataset\")\nMODELS_DIR = os.path.join(BASE_DIR, \"models\")\nCHECKPOINT_DIR = os.path.join(MODELS_DIR, \"seq2seq_checkpoints\")\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- decoding utils ---\n\ndef greedy_decode_sentence(urdu_sentence, model, src_tok, tgt_tok, max_len=80):\n    \"\"\"Greedy decoding for a single Urdu sentence.\"\"\"\n    model.eval()\n    with torch.no_grad():\n        sids = src_tok.encode(urdu_sentence)\n        if len(sids) == 0:\n            return \"\"\n        src = torch.tensor([sids], dtype=torch.long, device=device)\n        src_len = torch.tensor([len(sids)], dtype=torch.long, device=device)\n\n        preds = model.greedy_decode(src, src_len, max_tgt_len=max_len)\n        hyp_ids = preds[0].tolist()\n\n        # stop at EOS if exists\n        if TGT_EOS in hyp_ids:\n            hyp_ids = hyp_ids[:hyp_ids.index(TGT_EOS)]\n\n        out_str = tgt_tok.decode(hyp_ids)\n        return out_str.replace(\"</w>\", \"\").strip()\n\n\ndef beam_search_decode_sentence(urdu_sentence, model, src_tok, tgt_tok,\n                                beam_width=4, max_len=80):\n    \"\"\"Optimized beam search decoding with EOS safety.\"\"\"\n    model.eval()\n    with torch.no_grad():\n        sids = src_tok.encode(urdu_sentence)\n        if len(sids) == 0:\n            return \"\"\n\n        src_tensor = torch.tensor([sids], dtype=torch.long, device=device)\n        src_len = torch.tensor([len(sids)], dtype=torch.long, device=device)\n        enc_outputs, enc_h, enc_c = model.encoder(src_tensor, src_len)\n        enc_mask = (src_tensor != SRC_PAD)\n        B = 1\n        enc_h_flat = enc_h.permute(1, 0, 2).contiguous().view(B, -1)\n        enc_c_flat = enc_c.permute(1, 0, 2).contiguous().view(B, -1)\n        dec_h_flat = model.enc2dec_h(enc_h_flat)\n        dec_c_flat = model.enc2dec_c(enc_c_flat)\n        dec_h = dec_h_flat.view(model.dec_n_layers, B, model.dec_hid_dim).contiguous()\n        dec_c = dec_c_flat.view(model.dec_n_layers, B, model.dec_hid_dim).contiguous()\n\n        beams = [([TGT_SOS], 0.0, dec_h, dec_c)]\n        completed = []\n\n        for step in range(max_len):\n            new_beams = []\n            for tokens, score, h, c in beams:\n                if tokens[-1] == TGT_EOS:\n                    completed.append((tokens, score))\n                    continue\n\n                inp = torch.tensor([tokens[-1]], dtype=torch.long, device=device)\n                logits, h2, c2, _ = model.decoder.forward_step(inp, h, c, enc_outputs, enc_mask)\n                logp = F.log_softmax(logits.squeeze(0), dim=-1)\n                topk_vals, topk_ids = torch.topk(logp, beam_width)\n\n                for kid, val in zip(topk_ids.tolist(), topk_vals.tolist()):\n                    new_tokens = tokens + [kid]\n                    new_score = score + val\n                    new_beams.append((new_tokens, new_score, h2, c2))\n\n            if not new_beams:\n                break\n            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n\n            # stop early if enough EOS found\n            if len(completed) >= beam_width:\n                break\n\n        if completed:\n            completed = sorted(completed, key=lambda x: x[1], reverse=True)\n            best_tokens = completed[0][0]\n        else:\n            best_tokens = beams[0][0]\n\n        # strip special tokens\n        if TGT_EOS in best_tokens:\n            best_tokens = best_tokens[:best_tokens.index(TGT_EOS)]\n        if TGT_SOS in best_tokens:\n            best_tokens.remove(TGT_SOS)\n\n        out_str = tgt_tok.decode(best_tokens)\n        return out_str.replace(\"</w>\", \"\").strip()\n\n\n# ============================\n# âœ… Test on sample(s)\n# ============================\n\ntest_csv_path = os.path.join(DATA_DIR, \"test.csv\")\n\nif os.path.exists(test_csv_path):\n    test_df = pd.read_csv(test_csv_path, encoding=\"utf-8\")\n    print(f\"Loaded test dataset with {len(test_df)} samples.\\n\")\n\n    n_samples = min(5, len(test_df))  # test first 5\n    for i in range(n_samples):\n        urdu_text = str(test_df.iloc[i][\"urdu_clean\"])\n        ref = str(test_df.iloc[i].get(\"roman_clean\", \"\"))\n\n        start_time = time.time()\n        greedy_out = greedy_decode_sentence(urdu_text, model, char_tok_src, char_tok_tgt)\n        beam_out = beam_search_decode_sentence(urdu_text, model, char_tok_src, char_tok_tgt)\n        t = time.time() - start_time\n\n        print(f\"\\n--- SAMPLE {i+1}/{n_samples} --- ({t:.1f}s)\")\n        print(\"ğŸŸ© URDU  :\", urdu_text)\n        print(\"ğŸŸ¨ REF   :\", ref)\n        print(\"ğŸŸ¦ GREEDY:\", greedy_out)\n        print(\"ğŸŸª BEAM  :\", beam_out)\nelse:\n    print(\"No test.csv found â€” run manual test below instead.\")\n\n\n# ============================\n# âœ… Manual sentence test\n# ============================\n\nsample_sentence = \"Ù…ÛŒØ±Û’ Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª Ø³Ù†Ùˆ\"\nprint(\"\\n--- Manual Test ---\")\nprint(\"URDU :\", sample_sentence)\nprint(\"GREEDY:\", greedy_decode_sentence(sample_sentence, model, char_tok_src, char_tok_tgt))\nprint(\"BEAM  :\", beam_search_decode_sentence(sample_sentence, model, char_tok_src, char_tok_tgt))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T18:53:04.372353Z","iopub.execute_input":"2025-10-05T18:53:04.372940Z","iopub.status.idle":"2025-10-05T18:53:04.487739Z","shell.execute_reply.started":"2025-10-05T18:53:04.372913Z","shell.execute_reply":"2025-10-05T18:53:04.487002Z"}},"outputs":[{"name":"stdout","text":"No test.csv found â€” run manual test below instead.\n\n--- Manual Test ---\nURDU : Ù…ÛŒØ±Û’ Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª Ø³Ù†Ùˆ\nGREEDY: mere dil kÄ« baat sunÅ«\nBEAM  : mere dil kÄ« baat sunÅ«\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Cell 12: utilities for loading best checkpoint and inference\nimport glob\n\ndef load_best_checkpoint(checkpoint_dir=CHECKPOINT_DIR):\n    ckpts = sorted(glob.glob(os.path.join(checkpoint_dir, \"best_model_epoch*_bleu*.pt\")))\n    if not ckpts:\n        raise FileNotFoundError(f\"No checkpoints found in {checkpoint_dir}\")\n    best = ckpts[-1]\n    data = torch.load(best, map_location=device, weights_only=False)\n    enc = Encoder(input_dim=SRC_VOCAB_SIZE, emb_dim=EMB_DIM, hid_dim=HID_DIM, n_layers=ENC_LAYERS, dropout=DROPOUT, pad_idx=SRC_PAD)\n    dec = Decoder(output_dim=TGT_VOCAB_SIZE, emb_dim=EMB_DIM, enc_hid_dim=HID_DIM, dec_hid_dim=HID_DIM, n_layers=DEC_LAYERS, dropout=DROPOUT, pad_idx=TGT_PAD)\n    modelx = Seq2Seq(enc, dec, enc_hid_dim=HID_DIM, dec_hid_dim=HID_DIM, dec_n_layers=DEC_LAYERS).to(device)\n    modelx.load_state_dict(data[\"model_state\"])\n    modelx.eval()\n    print(\"Loaded checkpoint:\", best, \"val_bleu:\", data.get(\"val_bleu\"))\n    return modelx\n\n# example:\n# best_model = load_best_checkpoint()\n# print(greedy_decode_sentence(\"Ú¯Ù„Ø§Ø¨\", best_model, char_tok_src, char_tok_tgt))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T18:55:12.786435Z","iopub.execute_input":"2025-10-05T18:55:12.787371Z","iopub.status.idle":"2025-10-05T18:55:12.792907Z","shell.execute_reply.started":"2025-10-05T18:55:12.787340Z","shell.execute_reply":"2025-10-05T18:55:12.792152Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# ============================\n# âœ… Cell 13 â€” Urdu â†’ Roman Urdu Inference\n# ============================\n\nimport random\nimport torch\n\n# Ensure model and tokenizers are loaded\nbest_model = load_best_checkpoint()\nassert \"char_tok_src\" in locals() and \"char_tok_tgt\" in locals(), \"Tokenizers not loaded!\"\n\ndef translate_urdu(text, model=best_model, method=\"beam\", max_len=80):\n    \"\"\"\n    Translate Urdu â†’ Roman Urdu using greedy or beam decoding.\n    \"\"\"\n    text = text.strip()\n    if not text:\n        return \"\"\n    if method == \"beam\":\n        out = beam_search_decode_sentence(text, model, char_tok_src, char_tok_tgt, beam_width=4, max_len=max_len)\n    else:\n        out = greedy_decode_sentence(text, model, char_tok_src, char_tok_tgt, max_len=max_len)\n    return out.strip()\n\n\n# --- âœ… Manual single test ---\n#sample_text = \"Ù…ÛŒØ±Û’ Ø®ÙˆØ§Ø¨ Ø§Ø¯Ú¾ÙˆØ±Û’ ÛÛŒÚº\"\n#print(\"ğŸŸ© Urdu :\", sample_text)\n#print(\"ğŸŸ¦ Greedy:\", translate_urdu(sample_text, method=\"greedy\"))\n#print(\"ğŸŸª Beam  :\", translate_urdu(sample_text, method=\"beam\"))\n\n\n# --- âœ… Random sample tests (built-in Urdu lines) ---\nsample_lines = [\n    \"ÛŒÛ Ú©ÛŒØ§ Ø¨ÛŒÙˆÙ‚ÙˆÙ ÛÛ’\"\n]\n\nprint(\"\\n============================\")\nprint(\"ğŸŒ™ Random Urdu â†’ Roman Urdu Tests\")\nprint(\"============================\")\nfor line in random.sample(sample_lines, 1):\n    print(f\"\\nğŸŸ© Urdu: {line}\")\n    print(\"ğŸŸ¦ Greedy:\", translate_urdu(line, method=\"greedy\"))\n    print(\"ğŸŸª Beam  :\", translate_urdu(line, method=\"beam\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T19:02:13.450566Z","iopub.execute_input":"2025-10-05T19:02:13.451105Z","iopub.status.idle":"2025-10-05T19:02:13.621410Z","shell.execute_reply.started":"2025-10-05T19:02:13.451078Z","shell.execute_reply":"2025-10-05T19:02:13.620796Z"}},"outputs":[{"name":"stdout","text":"Loaded checkpoint: /kaggle/working/models/seq2seq_checkpoints/best_model_epoch9_bleu91.57.pt val_bleu: 91.57070189244088\n\n============================\nğŸŒ™ Random Urdu â†’ Roman Urdu Tests\n============================\n\nğŸŸ© Urdu: ÛŒÛ Ú©ÛŒØ§ Ø¨ÛŒÙˆÙ‚ÙˆÙ ÛÛ’\nğŸŸ¦ Greedy: ye kyÄ bavauqÅ«f hai\nğŸŸª Beam  : ye kyÄ bavauqÅ«f hai\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Cell 13: tiny augmentation helpers (duplicate with roman variants)\nimport itertools\n\ndef augment_roman_variants(roman_text):\n    # basic, low-rate augmentations: repeated vowels, alternate translit of long vowels\n    variants = set([roman_text])\n    # replace Ä-like sequences with aa and vice versa\n    variants.add(roman_text.replace(\"Ä\",\"aa\"))\n    variants.add(roman_text.replace(\"aa\",\"a\"))\n    # common replacements (add more as needed)\n    repls = [(\"kh\",\"x\"), (\"sh\",\"s h\"), (\"oo\",\"u\"), (\"ou\",\"u\")]\n    for a,b in repls:\n        variants.add(roman_text.replace(a,b))\n    return list(variants)\n\n# Example: augment train set by adding 1 variant per sample with low prob\naug_rows = []\nfor _, r in train_df.sample(frac=0.05, random_state=SEED).iterrows():  # augment 5% of data\n    v = augment_roman_variants(r[\"roman_clean\"])\n    if len(v) > 1:\n        aug_rows.append({\"urdu\": r[\"urdu_clean\"], \"roman\": v[1], \"urdu_clean\": r[\"urdu_clean\"], \"roman_clean\": v[1]})\n\nif aug_rows:\n    aug_df = pd.DataFrame(aug_rows)\n    aug_df.to_csv(os.path.join(DATA_DIR, \"augmented_samples.csv\"), index=False)\n    print(\"Created augmented samples:\", len(aug_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:44:58.607084Z","iopub.execute_input":"2025-10-05T06:44:58.607358Z","iopub.status.idle":"2025-10-05T06:44:58.648782Z","shell.execute_reply.started":"2025-10-05T06:44:58.607338Z","shell.execute_reply":"2025-10-05T06:44:58.648188Z"}},"outputs":[{"name":"stdout","text":"Created augmented samples: 512\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!zip -r /kaggle/working/working_dir.zip /kaggle/working/ -x \"/kaggle/working/state.db\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T19:34:28.116008Z","iopub.execute_input":"2025-10-05T19:34:28.116538Z","iopub.status.idle":"2025-10-05T19:35:15.282871Z","shell.execute_reply.started":"2025-10-05T19:34:28.116503Z","shell.execute_reply":"2025-10-05T19:35:15.281647Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/models/ (stored 0%)\n  adding: kaggle/working/models/roman_bpe_merges.json (deflated 74%)\n  adding: kaggle/working/models/urdu_bpe_vocab.json (deflated 75%)\n  adding: kaggle/working/models/roman_bpe_vocab.json (deflated 71%)\n  adding: kaggle/working/models/char_urdu_vocab.json (deflated 59%)\n  adding: kaggle/working/models/urdu_bpe_merges.json (deflated 77%)\n  adding: kaggle/working/models/char_roman_vocab.json (deflated 59%)\n  adding: kaggle/working/models/seq2seq_checkpoints/ (stored 0%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch15_bleu93.79.pt (deflated 8%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch9_bleu91.57.pt (deflated 8%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch20_bleu95.24.pt (deflated 8%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch16_bleu94.28.pt (deflated 8%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch7_bleu90.19.pt (deflated 8%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch17_bleu94.50.pt (deflated 8%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch8_bleu91.02.pt (deflated 8%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch11_bleu91.69.pt (deflated 8%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch12_bleu92.50.pt (deflated 8%)\n  adding: kaggle/working/models/seq2seq_checkpoints/best_model_epoch13_bleu93.49.pt (deflated 8%)\n  adding: kaggle/working/data/ (stored 0%)\n  adding: kaggle/working/data/augmented_samples.csv (deflated 76%)\n  adding: kaggle/working/data/pairs_clean.csv (deflated 78%)\n  adding: kaggle/working/data/pairs.csv (deflated 65%)\n  adding: kaggle/working/data/test.csv (deflated 77%)\n  adding: kaggle/working/data/val.csv (deflated 77%)\n  adding: kaggle/working/data/train.csv (deflated 77%)\n","output_type":"stream"}],"execution_count":18}]}